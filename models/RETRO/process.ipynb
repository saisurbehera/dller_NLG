{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google-cloud-cli-412.0.0-linux-x86_64.tar.gz  onstart.log  process.ipynb\n",
      "google-cloud-sdk\t\t\t      onstart.sh   text_closed_qa_train\n",
      "memorizing-transformers-pytorch\t\t      ports.log\n"
     ]
    }
   ],
   "source": [
    "!ls workspace/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'date'],\n",
       "        num_rows: 219289\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'date'],\n",
       "        num_rows: 28426\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "ds = load_from_disk(\"/workspace/text_closed_qa_train\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = ds[\"train\"][\"text\"] + ds[\"validation\"][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Radiohead - Music - New York Times LOS ANGELES - \"In Rainbows,\" the latest album from the British rock band Radiohead, has been readily available to music fans for almost three months, first as a digital download in an unconventional tip-jar offering in which fans decided for themselves what to pay for it, and then, like most pop music today, as digital files circulating on free, unlicensed file-swapping networks. One matter remains: Will anyone buy the CD? Starting on Tuesday, the album, in plastic disc form, is on sale in record shops (this time with a list price, $13.98, that is not subject to consumer whims). Though hard-core fans almost surely have acquired the album, one way or another, Radiohead had plans to promote the CD release with a \"prerecording\" of the band performing songs from \"In Rainbows\" on the www.radiohead.tv Web site starting on Monday, according to the band\\'s Web site, radiohead.com. It is also to be shown on satellite and cable systems that carry the Current TV channel. Though hailed by critics, the album is seen as an uncertain prospect commercially. That is because the band has declined to say how many copies have been distributed since October, when it diverged from industry custom and offered a digital download of \"In Rainbows\" for however much fans chose to pay - even nothing. Since then the band\\'s representatives have described the offering as, among other things, a way of testing whether digital downloads eat into sales of CDs. Radiohead chose to release the CD through the independent label ATO Records and its imprint TBD. The band\\'s 2003 album \"Hail to the Thief\" was the last one under its recording contract with the music giant EMI. ATO is shipping an estimated 400,000 copies of the album to record shops, said executives briefed on the label\\'s plans. It is not uncommon for shops to sell half of the shipment of a big album in its first week on sale. But Radiohead\\'s performance may differ; not only has the album been widely available online, but it is also hitting record shops in the traditionally slow post-holiday sales period. As a result, it is seen as a long shot that the band could match the performance of \"Hail to the Thief,\" which sold 300,000 copies in its first week after going on sale in June 2003, and went on to sell a total of roughly 1 million copies, according to Nielsen SoundScan data. Will Botwin, the president of ATO, said that in spite of the availability of Radiohead\\'s music online, many fans might still have reason to pay for the CD version. \"You\\'ve still got hard-core Radiohead fans that are very inclined to own anything they can from the band,\" he said. He added that the critical praise and international headlines generated by the band\\'s release plan may have drawn new fans. \"I think you\\'re going to find a new buyer that might 01 January 2008'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "247715it [00:43, 5736.14it/s] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for i,j in tqdm(enumerate(all_text)):\n",
    "    with open(\"/workspace/text_files/\"+str(i) + \".txt\", \"w\") as text_file:\n",
    "        text_file.write( j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mbin\u001b[0m/   \u001b[01;34mhome\u001b[0m/   \u001b[01;34mmnt\u001b[0m/   \u001b[01;34mrun\u001b[0m/   \u001b[30;42mtmp\u001b[0m/               \u001b[01;34musr\u001b[0m/\n",
      "\u001b[01;34mboot\u001b[0m/  \u001b[01;34mlib\u001b[0m/    \u001b[01;34mopt\u001b[0m/   \u001b[01;34msbin\u001b[0m/  train.chunks.dat   \u001b[01;34mvar\u001b[0m/\n",
      "\u001b[01;34mdev\u001b[0m/   \u001b[01;34mlib64\u001b[0m/  \u001b[01;34mproc\u001b[0m/  \u001b[01;34msrv\u001b[0m/   train.doc_ids.dat  \u001b[01;34mworkspace\u001b[0m/\n",
      "\u001b[01;34metc\u001b[0m/   \u001b[01;34mmedia\u001b[0m/  \u001b[01;34mroot\u001b[0m/  \u001b[01;34msys\u001b[0m/   train.seq.dat\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RETRO.retro_pytorch.retrieval import text_folder_to_chunks_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 108942.96it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for i in tqdm(range(10)):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/247715 [00:00<?, ?it/s]Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (645 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 247715/247715 [05:48<00:00, 711.05it/s] \n"
     ]
    }
   ],
   "source": [
    "stats = text_folder_to_chunks_(\n",
    "    folder = './workspace/text_files/',\n",
    "    glob = '**/*.txt',\n",
    "    chunks_memmap_path = './train.chunks.dat',\n",
    "    seqs_memmap_path = './train.seq.dat',\n",
    "    doc_ids_memmap_path = './train.doc_ids.dat',  # document ids are needed for filtering out neighbors belonging to same document appropriately during computation of nearest neighbors\n",
    "    chunk_size = 64,\n",
    "    seq_len = 512,\n",
    "    max_chunks = 1000000,\n",
    "    max_seqs = 1000000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mbin\u001b[0m/   \u001b[01;34mhome\u001b[0m/   \u001b[01;34mmnt\u001b[0m/   \u001b[01;34mrun\u001b[0m/   \u001b[30;42mtmp\u001b[0m/                       train.seq.dat\n",
      "\u001b[01;34mboot\u001b[0m/  \u001b[01;34mlib\u001b[0m/    \u001b[01;34mopt\u001b[0m/   \u001b[01;34msbin\u001b[0m/  train.chunks.dat           \u001b[01;34musr\u001b[0m/\n",
      "\u001b[01;34mdev\u001b[0m/   \u001b[01;34mlib64\u001b[0m/  \u001b[01;34mproc\u001b[0m/  \u001b[01;34msrv\u001b[0m/   train.chunks.dat.embedded  \u001b[01;34mvar\u001b[0m/\n",
      "\u001b[01;34metc\u001b[0m/   \u001b[01;34mmedia\u001b[0m/  \u001b[01;34mroot\u001b[0m/  \u001b[01;34msys\u001b[0m/   train.doc_ids.dat          \u001b[01;34mworkspace\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mbin\u001b[0m/   \u001b[01;34mhome\u001b[0m/   \u001b[01;34mmnt\u001b[0m/   \u001b[01;34mrun\u001b[0m/   \u001b[30;42mtmp\u001b[0m/                       train.seq.dat\n",
      "\u001b[01;34mboot\u001b[0m/  \u001b[01;34mlib\u001b[0m/    \u001b[01;34mopt\u001b[0m/   \u001b[01;34msbin\u001b[0m/  train.chunks.dat           \u001b[01;34musr\u001b[0m/\n",
      "\u001b[01;34mdev\u001b[0m/   \u001b[01;34mlib64\u001b[0m/  \u001b[01;34mproc\u001b[0m/  \u001b[01;34msrv\u001b[0m/   train.chunks.dat.embedded  \u001b[01;34mvar\u001b[0m/\n",
      "\u001b[01;34metc\u001b[0m/   \u001b[01;34mmedia\u001b[0m/  \u001b[01;34mroot\u001b[0m/  \u001b[01;34msys\u001b[0m/   train.doc_ids.dat          \u001b[01;34mworkspace\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py38/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedded 16 / 1000\n",
      "embedded 32 / 1000\n",
      "embedded 48 / 1000\n",
      "embedded 64 / 1000\n",
      "embedded 80 / 1000\n",
      "embedded 96 / 1000\n",
      "embedded 112 / 1000\n",
      "embedded 128 / 1000\n",
      "embedded 144 / 1000\n",
      "embedded 160 / 1000\n",
      "embedded 176 / 1000\n",
      "embedded 192 / 1000\n",
      "embedded 208 / 1000\n",
      "embedded 224 / 1000\n",
      "embedded 240 / 1000\n",
      "embedded 256 / 1000\n",
      "embedded 272 / 1000\n",
      "embedded 288 / 1000\n",
      "embedded 304 / 1000\n",
      "embedded 320 / 1000\n",
      "embedded 336 / 1000\n",
      "embedded 352 / 1000\n",
      "embedded 368 / 1000\n",
      "embedded 384 / 1000\n",
      "embedded 400 / 1000\n",
      "embedded 416 / 1000\n",
      "embedded 432 / 1000\n",
      "embedded 448 / 1000\n",
      "embedded 464 / 1000\n",
      "embedded 480 / 1000\n",
      "embedded 496 / 1000\n",
      "embedded 512 / 1000\n",
      "embedded 528 / 1000\n",
      "embedded 544 / 1000\n",
      "embedded 560 / 1000\n",
      "embedded 576 / 1000\n",
      "embedded 592 / 1000\n",
      "embedded 608 / 1000\n",
      "embedded 624 / 1000\n",
      "embedded 640 / 1000\n",
      "embedded 656 / 1000\n",
      "embedded 672 / 1000\n",
      "embedded 688 / 1000\n",
      "embedded 704 / 1000\n",
      "embedded 720 / 1000\n",
      "embedded 736 / 1000\n",
      "embedded 752 / 1000\n",
      "embedded 768 / 1000\n",
      "embedded 784 / 1000\n",
      "embedded 800 / 1000\n",
      "embedded 816 / 1000\n",
      "embedded 832 / 1000\n",
      "embedded 848 / 1000\n",
      "embedded 864 / 1000\n",
      "embedded 880 / 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-15 18:13:08,384 [INFO]: Using 48 omp threads (processes), consider increasing --nb_cores if you have more\n",
      "2022-12-15 18:13:08,385 [INFO]: Launching the whole pipeline 12/15/2022, 18:13:08\n",
      "2022-12-15 18:13:08,386 [INFO]: Reading total number of vectors and dimension 12/15/2022, 18:13:08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedded 896 / 1000\n",
      "embedded 912 / 1000\n",
      "embedded 928 / 1000\n",
      "embedded 944 / 1000\n",
      "embedded 960 / 1000\n",
      "embedded 976 / 1000\n",
      "embedded 992 / 1000\n",
      "embedded 1000 / 1000\n",
      "saved .tmp/embeddings/00000.npy\n",
      "saved .tmp/embeddings/00001.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 3265.32it/s]\n",
      "2022-12-15 18:13:08,430 [INFO]: There are 1000 embeddings of dim 768\n",
      "2022-12-15 18:13:08,430 [INFO]: >>> Finished \"Reading total number of vectors and dimension\" in 0.0440 secs\n",
      "2022-12-15 18:13:08,431 [INFO]: \tCompute estimated construction time of the index 12/15/2022, 18:13:08\n",
      "2022-12-15 18:13:08,432 [INFO]: \t\t-> Train: 16.7 minutes\n",
      "2022-12-15 18:13:08,434 [INFO]: \t\t-> Add: 0.0 seconds\n",
      "2022-12-15 18:13:08,435 [INFO]: \t\tTotal: 16.7 minutes\n",
      "2022-12-15 18:13:08,436 [INFO]: \t>>> Finished \"Compute estimated construction time of the index\" in 0.0036 secs\n",
      "2022-12-15 18:13:08,436 [INFO]: \tChecking that your have enough memory available to create the index 12/15/2022, 18:13:08\n",
      "2022-12-15 18:13:08,438 [INFO]: 3.3MB of memory will be needed to build the index (more might be used if you have more)\n",
      "2022-12-15 18:13:08,440 [INFO]: \t>>> Finished \"Checking that your have enough memory available to create the index\" in 0.0027 secs\n",
      "2022-12-15 18:13:08,441 [INFO]: \tSelecting most promising index types given data characteristics 12/15/2022, 18:13:08\n",
      "2022-12-15 18:13:08,441 [INFO]: \t>>> Finished \"Selecting most promising index types given data characteristics\" in 0.0000 secs\n",
      "2022-12-15 18:13:08,442 [INFO]: \tCreating the index 12/15/2022, 18:13:08\n",
      "2022-12-15 18:13:08,442 [INFO]: \t\t-> Instanciate the index HNSW15 12/15/2022, 18:13:08\n",
      "2022-12-15 18:13:08,446 [INFO]: \t\t>>> Finished \"-> Instanciate the index HNSW15\" in 0.0027 secs\n",
      "2022-12-15 18:13:08,447 [INFO]: \t\t-> Adding the vectors to the index 12/15/2022, 18:13:08\n",
      "2022-12-15 18:13:08,447 [INFO]: The memory available for adding the vectors is 1021.0MB(total available - used by the index)\n",
      "2022-12-15 18:13:08,448 [INFO]: Using a batch size of 325520 (memory overhead 953.7MB)\n",
      "100%|██████████| 2/2 [00:00<00:00, 39.23it/s]\n",
      "2022-12-15 18:13:08,509 [INFO]: \tComputing best hyperparameters for index /workspace/RETRO/.tmp/.index/knn.index 12/15/2022, 18:13:08\n",
      "2022-12-15 18:13:13,326 [INFO]: \t>>> Finished \"Computing best hyperparameters for index /workspace/RETRO/.tmp/.index/knn.index\" in 4.8162 secs\n",
      "2022-12-15 18:13:13,328 [INFO]: The best hyperparameters are: efSearch=16\n",
      "2022-12-15 18:13:13,328 [INFO]: \tCompute fast metrics 12/15/2022, 18:13:13\n",
      " 50%|█████     | 1/2 [00:00<00:00, 98.88it/s]\n",
      "2022-12-15 18:13:23,349 [INFO]: \t>>> Finished \"Compute fast metrics\" in 10.0205 secs\n",
      "2022-12-15 18:13:23,350 [INFO]: \tSaving the index on local disk 12/15/2022, 18:13:23\n",
      "2022-12-15 18:13:23,353 [INFO]: \t>>> Finished \"Saving the index on local disk\" in 0.0024 secs\n",
      "2022-12-15 18:13:23,354 [INFO]: \t\t>>> Finished \"-> Adding the vectors to the index\" in 14.9069 secs\n",
      "2022-12-15 18:13:23,355 [INFO]: {\n",
      "2022-12-15 18:13:23,355 [INFO]: \tindex_key: HNSW15\n",
      "2022-12-15 18:13:23,355 [INFO]: \tindex_param: efSearch=16\n",
      "2022-12-15 18:13:23,356 [INFO]: \tindex_path: /workspace/RETRO/.tmp/.index/knn.index\n",
      "2022-12-15 18:13:23,356 [INFO]: \tsize in bytes: 3208150\n",
      "2022-12-15 18:13:23,420 [INFO]: \tavg_search_speed_ms: 11.415915068671676\n",
      "2022-12-15 18:13:23,420 [INFO]: \t99p_search_speed_ms: 74.0071342443116\n",
      "2022-12-15 18:13:23,421 [INFO]: \treconstruction error %: 0.0\n",
      "2022-12-15 18:13:23,421 [INFO]: \tnb vectors: 1000\n",
      "2022-12-15 18:13:23,421 [INFO]: \tvectors dimension: 768\n",
      "2022-12-15 18:13:23,422 [INFO]: \tcompression ratio: 0.9575612112899958\n",
      "2022-12-15 18:13:23,423 [INFO]: }\n",
      "2022-12-15 18:13:23,423 [INFO]: \t>>> Finished \"Creating the index\" in 14.9804 secs\n",
      "2022-12-15 18:13:23,423 [INFO]: >>> Finished \"Launching the whole pipeline\" in 15.0374 secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knns calculated for 500 / 1000\n",
      "knns calculated for 1000 / 1000\n",
      "knn saved to /workspace/all_d/train.chunks.knn.dat\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(PosixPath('/workspace/all_d/train.chunks.knn.dat'),\n",
       " <faiss.swigfaiss.IndexHNSWFlat; proxy of <Swig Object of type 'faiss::IndexHNSWFlat *' at 0x7f039034a4b0> >)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from retro_pytorch.retrieval import chunks_to_precalculated_knn_\n",
    "chunks_to_precalculated_knn_(\n",
    "    num_chunks = 1000,\n",
    "    chunk_size = 64,\n",
    "    chunk_memmap_path = '/workspace/all_d/train.chunks.dat',    # path to main chunks dataset\n",
    "    doc_ids_memmap_path = '/workspace/all_d/train.doc_ids.dat', # path to document ids created by text_folder_to_chunks_, used for filtering out neighbors that belong to the same document\n",
    "    num_nearest_neighbors = 4,                   # number of nearest neighbors you'd like to use\n",
    "    num_extra_neighbors = 10                     # fetch 10 extra neighbors, in the case that fetched neighbors are frequently from same document (filtered out)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "3ea19d11efa7602c1f12500925a974ed4f31fcf847bd6f694bd5180da2602ded"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
